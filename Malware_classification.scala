import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf
import java.util.Properties
import org.apache.spark.mllib.classification.NaiveBayes
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.LabeledPoint
import org.apache.spark.mllib.tree.DecisionTree
import org.apache.spark.mllib.util.MLUtils

var trainfiles = sc.textFile("hdfs://cshadoop1/train_Full_Labels") //specify the location of train_Full_Labels here

val File_Class = trainfiles.map(x => (x.split(",")(0).replace('"',' ').trim()->x.split(",")(1).toInt)).collect//creating a map with filename as key and class name as value

var data1gramFiles = "hdfs://cshadoop1.utdallas.edu/malware/1gram/"
var data2gramFiles = "hdfs://cshadoop1.utdallas.edu/malware/2gram/"
var data3gramFiles = "hdfs://cshadoop1.utdallas.edu/malware/3gram/"
     
// ** Finding out the unique words and saving it in uwords **
var train_FNames = File_Class.map{case(x,y)=>x}
var train_FNames_dt = File_Class.map{case(x,y)=>(x,y-1)}.collect
val label = File_Class.toMap
val label_dt = train_FNames_dt.toMap
var uwords=Array[String]()
var k=0
var lines ="" 
def getuwords(filename:String):String = {
   val lines = sc.textFile(data1gramFiles+filename+".bytes")
   val arr = lines.flatMap(line=> line.split(" ")).filter(x=>x.size<7).collect
   val u = arr.toSet.toArray
   return u.mkString("\t")
}

val uniq = train_FNames.take(10).map(line => getuwords(line)).flatMap(line=> line.split("\t")).map(word =>(word,1))
val uniq2 = sc.parallelize(uniq).reduceByKey(_+_).sortBy(_._2,false).top(1000)
var uniq3 = uniq2
var x = 1
for(i <- uniq2){
uniq3(x-1)= (i._1,x)
x = x+1
}

val freq1 = uniq2.toMap
val freq2 = uniq3.toMap

def getfreq(filename: String):String = {
	val lines = sc.textFile(data1gramFiles+filename+".bytes")
	val arr = lines.flatMap(line => line.split(" ")).filter(x => freq1.contains(x)).map(a=>(a,1)).reduceByKey(_+_).collectAsMap()
	val arr2 = uniq2.map(line => if(arr.contains(line._1)) arr(line._1) else 0)
	return label(filename)+ "," + arr2.mkString("\t")
}

def getfreq2(filename: String):String = {
        val lines = sc.textFile(data1gramFiles+filename+".bytes")
        val arr = lines.flatMap(line => line.split(" ")).filter(x => freq1.contains(x)).map(a=>(a,1)).reduceByKey(_+_).collectAsMap()
        val arr2 = uniq2.map(line => if(arr.contains(line._1)) freq2(line._1) + ":" + arr(line._1) else freq2(line._1) + ":" + 0)
        return label_dt(filename)+ " " + arr2.mkString(" ")
}


//uniq2.foreach(println).saveAsTextFile("Malware/uwords")

val freq = train_FNames.take(10).map(line => getfreq(line))
//freq.saveAsTextFile("/Malware/freq")
val data = sc.parallelize(freq)
val parsedData = data.map { line =>
  val parts = line.split(',')
  LabeledPoint(parts(0).toDouble, Vectors.dense(parts(1).split('\t').map(_.toDouble)))
}
// Split data into training (60%) and test (40%).
val splits = parsedData.randomSplit(Array(0.6, 0.4), seed = 11L)
val training = splits(0)
val test = splits(1)

val model = NaiveBayes.train(training, lambda = 1.0)

val predictionAndLabel = test.map(p => (model.predict(p.features), p.label))
val accuracy = 1.0 * predictionAndLabel.filter(x => x._1 == x._2).count() / test.count()

import org.apache.spark.mllib.tree.DecisionTree
import org.apache.spark.mllib.util.MLUtils

val freq2 = train_FNames.take(10).map(line => getfreq2(line))
val data2 = sc.parallelize(freq2)
data2.saveAsTextFile("/yxw124430/inputofdt")
val data2 = MLUtils.loadLibSVMFile(sc, "hdfs://cshadoop1/yxw124430/inputofdt")
val splits = data2.randomSplit(Array(0.7, 0.3))
val (trainingData, testData) = (splits(0), splits(1))

val numClasses = 9
val categoricalFeaturesInfo = Map[Int, Int]()
val impurity = "gini"
val maxDepth = 5
val maxBins = 32

val model = DecisionTree.trainClassifier(trainingData, numClasses, categoricalFeaturesInfo,
  impurity, maxDepth, maxBins)

  // Evaluate model on test instances and compute test error
  val labelAndPreds = testData.map { point =>
    val prediction = model.predict(point.features)
      (point.label, prediction)
      }
val testErr = labelAndPreds.filter(r => r._1 != r._2).count.toDouble / testData.count()
println("Test Error = " + testErr)
println("Learned classification tree model:\n" + model.toDebugString)



